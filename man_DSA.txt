Scoops:


Finding smallest primes in a large range of numbers: http://en.wikipedia.org/wiki/Sieve_of_Eratosthenes:
    take a number and keep marking all its multiples. As we go forward, what ever is found unmarked, is a prime

Quad Tree:
    When you have to maintain a square which can be devided into subsquares recursively but not homogeneusly
    http://en.wikipedia.org/wiki/Quadtree

Dictionary: its a concept of key-value pair. not a data structure. can be implemented using any appropriate data structure
    http://stackoverflow.com/questions/10017808/best-data-structure-for-dictionary-implementation
    a good read on the large answer
    introduces: succinct trees (low memory dictionary implementation) http://stevehanov.ca/blog/index.php?id=120

Finding kth largest
    keep record of the number of children a node has (in addition to the left pointer and the right pointer).
    now we have been asked to find the 50th largest. we are at a node and we see that my right child has 15 children.
    all 15 children and this node are more than me. so lets traverse to the left node and look for the 35th largest node


Sorting a Linked List: A good discussion
    merge would be most efective. its inplace. O(nlogn).
    caveat: breaking the list into two roughly equal parts may not be feasible in case the list is
    arbitrarily large. traversing whole list would invite n potential cache misses
    somewhat awesome solution: sort first 2 elements. sort next two elements.
                               sort the first two and next two using merge sort. 4 elements got sorted.
			       sort similarly the next 4 and merge the first 4 and next 4 elements. get 8 sorted elements
			       ... (implement this).
			       O(nlogn) comparisons. stack would use O(logn) space due to recursion
    how is cache misses reduced. we are anyway doing nlogn traveral ultimately.
    check: http://jonisalonen.com/2013/sorting-a-linked-list/

    A linked list can have memory allocation anywhere in the virtual address space..
    iterating through an LL can be very expensive in paging / caching.
    sorting a linked list is not a good idea. be it merge sort or bubble or counting sort.
    probably in a huge dataset, copying the whole set into an array, purge the LL (space saved)
    then sort the array (better memory localization)
    practically this turns out to be faster on normal PCs much due to ..
    localization on a million elements with high fragmentation
    check: http://stackoverflow.com/questions/1525117/whats-the-fastest-algorithm-for-sorting-a-linked-list
    packed list and fragmented list ??

    Or just copy the linked list into a new list. rather recreate the linked list into a new
    Data Structure like a BST (simplest case, or another list) and keep on sorting tree as each element is inserted.
    simultaneously keep freeing the corresponding element from the older list

    Radix + Counting sort:
    a reasonably good way of sorting integers:
    combination of Radix sort and counting sort.
    sort each decimal place of a radix sort using counting sort
    Primer: Radix sort: sort last decimal place first, then the 10th place digit and go on
            Counting sort: arrange all numbers into an ascending hashtag and after hashing each entry, sorted

    heaps: min heaps, max heaps
    heapify: O(log n)
    create / build heap: O(n)
    heap sort: O(nlog n)

    inplace, O(n) ?? really


Unrolled Linked List: stores multple elements in a node for somewhat better locality of memory.

tuning performance on external sorts:
    parallel processing
    parallel reads writes to hard disk
    increasing hardware speed, more ram, solid state drives
    increasing software speed, optiming by using different types of sorting on different passes


BST insertion complexity is O(n^2)   in case of a  badly balanced tree where the tree turns into a linked list
                            O(nlogn) in case of a fairly balanced tree


Searching a smaller string into a bigger string.
    Use iteration to go throug the first characters in the bigger string. and once found a match use recursion to
    determine if the subsequent characters match. Should work efficiently in cases where we are searching
    ssshit in holysssssshittt.


Tree traversals. Find maximum depth of a tree. Do any of the depth first searches.
    In tree traversal inorder postorder and preorders all are different kinds of depth first searches.
    in postorder the observation is that a traversal while returning always returns to its parent.
    Hence if we are asked to traverse a tree without recursion, we may have to implement a stack by ourself, then
    postorder should be a good way to speed up returning calls
    (better and serialized nodes locality while poping from stack)

Red Black Trees and AVL Trees:
    red black trees and avl trees both are balanced but red black trees allows for
    fast insertion and deletion with guarantee, hence useful for time sensitive applications like
    completely fair schedulers. 
    AVL trees allows for faster retrieval without guarantee, it's a more rigidly balanced tree,
    hence used when the data structure is created once like a dictionary.


Counting set bits in a number:
huge discussion:
    http://stackoverflow.com/questions/109023/how-to-count-the-number-of-set-bits-in-a-32-bit-integer



hex(input as string) to decimal.
say input is 0xABAB.
have a 256 byte array. array index 55 ['7'] would contain 7
                       array index 56 ['8'] would contain 8
                       array index 57 ['9'] would contain 9  ...
                       array index 65 ['A'] would contain 10
                       array index 66 ['B'] would contain 11
                       array index 67 ['C'] would contain 12
                       array index 68 ['D'] would contain 13
                       array index 69 ['E'] would contain 14 ...
                       array index 97 ['a'] would contain 10
                       array index 98 ['b'] would contain 11
                       array index 99 ['c'] would contain 12 ...
go through the string. for every byte, lookup the array for the corresponding value.
shift left the previous value 4 bits and OR the new value. finally we get the decimal value
Only 16 + 6 places would have useful values all other indices would be wasted.


Multiplying polynomials:
	use a 2D matrix. put polynomial co-efficients on each axis. multiply and put the values in the
	different cells of the matrix. add the cell values diagonally from SW to NE. each SW to NE path would
	have coefficients for a perticular power of x



In place merge sorting a two phase sorted array with the following pattern:
A1A2A3A4A5...B1B2B3B4B5...
	this is pretty simple because given an element we can calculate the initial and final position
	of the element. hence just pick an element and move it to its final position. If the final position
	is already occupied move that element again. If the final position is not already occupied,
	pick the next element and continue with this algorithm. In the end all elements should have moved
	to their final position. I think this should not get into any infinite loops or undefined state since
	the posiitons of the elements are fully deterministic just by their values. The algo fails if we are
	dealing with numbers though in that case looking at a number we cannot decide its final position.
	That boils down to a inplace merge sort algorithm which still doesnt have a stable solution.
	But if we can somehow predict the final position of an element (say after an initial swipe) that should
	be a solvable problem.
	Also the  problem gets really simple if we are allowed to use n extra space for an array of 2n (A1 to Bn)
	Any other approach for both the above scenarios ??






Amortized Complexity:
Amortized complexity is more realistic than worst case asymptotic analysis. AMortized complexity is
mostly less (if not equal) than worst case asymptotic complexities
scripting languages (for instance) are often happy to grow arrays and hash tables by some factor
even though that is an expensive operation. The growing can a O(n) operation, but amortized is O(1) because you do it rarely.

an unofficial explaination form stackoverflow:
	For example searching an unsorted array of n items for a single match may take up to n comparisons and
	hence is o(n) complexity. However, if we know the same array is going to be searched for m items,
	repeating the total task would then have complexity O(m*n). However, if we sort the array in advance,
	the cost is O(n log(n)), and successive searches take only O(log(n)) for a sorted array. Thus
	the total amortised cost for m elements taking this approach is O(n*log(n) + m*log(n)).
	If m >= n, this equates to O(n log(n)) by pre-sorting compared to O(n^2) for not sorting. Thus
	the amortised cost is cheaper. In a way we see that for a large data set, the number of sheap operations
	are so large that we can kind of ignore the occasional heavy operations to find the cost which is the amortized cost
	http://stackoverflow.com/questions/11102585/what-is-amortized-analysis-of-algorithms

It gets very essential to consider amortized costs when analuzing dynamic arrays and other such data structures where an
expensive operation is done occasionally in case of a large dataset

more: ??
















links:
http://jonisalonen.com/2013/sorting-a-linked-list/
http://stackoverflow.com/questions/1525117/whats-the-fastest-algorithm-for-sorting-a-linked-list
http://stackoverflow.com/questions/109023/how-to-count-the-number-of-set-bits-in-a-32-bit-integer
http://stackoverflow.com/questions/11102585/what-is-amortized-analysis-of-algorithms
http://people.csail.mit.edu/bdean/6.046/dp/
http://en.wikipedia.org/wiki/Sieve_of_Eratosthenes
http://en.wikipedia.org/wiki/Quadtree
http://stackoverflow.com/questions/10017808/best-data-structure-for-dictionary-implementation
http://stevehanov.ca/blog/index.php?id=120

Stuff to learn:
===============
natural sort:
Merge sort: get the algo better
Skip Lists: learn again
a good DFS and BFS implementation
splay trees

DP: http://people.csail.mit.edu/bdean/6.046/dp/



















